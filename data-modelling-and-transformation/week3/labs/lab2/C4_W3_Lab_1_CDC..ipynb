{
  "metadata": {
    "name": "C4_W3_Lab_1_CDC",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Week 3 Lab: Capture Data Change with Flink and Debezium\n\nFollow the instructions in this notebook to work with the Flink environment."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1 - Configuring the Flink Environment\n\nStart by configuring the Flink enviroment. Run the following cell:"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.conf\n# You need to run this paragraph first before running any flink code.\nflink.execution.packages\torg.apache.flink:flink-connector-kafka_2.11:1.14.0,org.apache.flink:flink-connector-kafka-base_2.11:1.11.6,org.apache.flink:flink-json:1.14.0,org.apache.flink:flink-avro-confluent-registry:1.14.0,mysql:mysql-connector-java:8.0.28,org.apache.flink:flink-connector-jdbc_2.11:1.14.6,org.postgresql:postgresql:42.7.3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "To test the Interpreters, run the following queries with the `%mysql` and `%psql` interpreters to check the available schemas and tables:"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nshow schemas;"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nselect table_schema as database_name,\n    table_name\nfrom information_schema.tables\nwhere table_type \u003d \u0027BASE TABLE\u0027\n        and table_schema \u003d \u0027classicmodels\u0027\norder by database_name, table_name;"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nSELECT schema_name\nFROM information_schema.schemata;"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%psql\nselect * from pg_tables where schemaname\u003d\u0027classicmodels_star_schema\u0027;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2 - Customers Dimension\n\nYou will use Flink SQL to connect to the Debezium/Kafka topics and create Flink tables on top of them, these topics bring the current values and future changes for each table in the source database. Then you will create Flink tables connecting to the tables in the target database and write the SQL logic to insert from the Source tables into the Output tables.\n\nBefore defining the CDC pipeline, review the original `customers` table from the source system, use the `%mysql` interpreter to run the following cell:"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nSELECT * FROM classicmodels.customers limit 10;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.1 - Creating Flink Tables\n\nTo create Flink tables with the SQL API, you will use `CREATE` statements that register a table into the current Flink catalog using the `%flink.ssql` Interpreter. You define the schema of the source data and then specify the **connector** format and parameters of the table.\n\nThis is an example of a table named `source_customers` with a Kafka topic:\n\n```sql\nCREATE TABLE source_customers (\n    ...\n) WITH (\n  \u0027connector\u0027 \u003d \u0027kafka\u0027\n  \u0027topic\u0027 \u003d \u0027\u003cKAFKA_TOPIC\u003e\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027\u003cKAFKA_SERVER_URL\u003e\u0027\n  ...\n);\n```\n\nThe data will be coming from a Kafka topic connected to Debezium and the source system. You will need to use the following configuration parameters for the table connection:\n\n```sql\nCREATE TABLE source_customers (\n    ...\n) WITH (\n  \u0027connector\u0027 \u003d \u0027kafka\u0027,\n  \u0027topic\u0027 \u003d \u0027dbserver1.classicmodels.customers\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027kafka:9092\u0027,\n  \u0027properties.group.id\u0027 \u003d \u0027testGroup\u0027,\n  \u0027format\u0027 \u003d \u0027debezium-json\u0027,\n  \u0027scan.startup.mode\u0027 \u003d \u0027earliest-offset\u0027,\n  \u0027debezium-json.schema-include\u0027 \u003d \u0027true\u0027\n);\n```"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2.1.1. In the code cell below add those connection parameters and review the list of the columns. Note, that in this particular case, a metadata column will be added to the original source schema. This allows to access information obtained or produced by the connector, in this case, the timestamp of the Kafka record in the topic. To read and write a Kafka record\u0027s timestamp in the additional metadata column, the following code is used:\n\n```sql\n`event_time` TIMESTAMP(3) METADATA FROM \u0027timestamp\u0027,\n```"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\nDROP TABLE IF EXISTS source_customers;\n\nCREATE TABLE source_customers (\n    `customerNumber` INT,\n    `customerName` STRING,\n    `contactLastName` STRING,\n    `contactFirstName` STRING,\n    `phone` STRING,\n    `addressLine1` STRING,\n    `addressLine2` STRING,\n    `city` STRING,\n    `state` STRING,\n    `postalCode` STRING,\n    `country` STRING,\n    `salesRepEmployeeNumber` INT,\n    `creditLimit` FLOAT, --VARBINARY,\n    `event_time` TIMESTAMP(3) METADATA FROM \u0027timestamp\u0027,\n    PRIMARY KEY (`customerNumber`)  NOT ENFORCED\n) WITH (\n  \u0027connector\u0027 \u003d \u0027kafka\u0027,\n  \u0027topic\u0027 \u003d \u0027dbserver1.classicmodels.customers\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027kafka:9092\u0027,\n  \u0027properties.group.id\u0027 \u003d \u0027testGroup\u0027,\n  \u0027format\u0027 \u003d \u0027debezium-json\u0027,\n  \u0027scan.startup.mode\u0027 \u003d \u0027earliest-offset\u0027,\n  \u0027debezium-json.schema-include\u0027 \u003d \u0027true\u0027\n);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2.1.2. To check that the `source_customers` table is well defined, add this table name into the following `SELECT` query and run the cell. After getting some results cancel the query in the upper right of the cell with the pause icon."
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%flink.ssql\nSELECT *\nFROM source_customers;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2.1.3. You can use all types of connectors, for example a JDBC one to connect with SQL databases:\n\n```sql\nCREATE TABLE TargetTable (\n    ...\n) WITH (\n  \u0027connector\u0027 \u003d \u0027jdbc\u0027,\n  \u0027url\u0027 \u003d \u0027\u003cJDBC_URL\u003e\u0027,\n  \u0027table-name\u0027 \u003d \u0027\u003cTARGET_SCHEMA\u003e.\u003cTARGET_TABLE\u003e\u0027,\n  \u0027username\u0027 \u003d \u0027\u003cDATABASE_USER\u003e\u0027,\n  \u0027password\u0027 \u003d \u0027\u003cDATABASE_PASSWORD\u003e\u0027\n);\n```\n\nLet\u0027s now define the `dest_customers` output table connecting to an existing table in the Postgres RDS target database, specifically the `classicmodels_star_schema.dim_customers` table. Use this configuration for the target table connection:\n\n```sql\nCREATE TABLE dest_customers (\n    ...\n) WITH (\n  \u0027connector\u0027 \u003d \u0027jdbc\u0027,\n  \u0027url\u0027 \u003d \u0027jdbc:postgresql://\u003cPOSTGRES_ENDPOINT\u003e:5432/postgres\u0027,\n  \u0027table-name\u0027 \u003d \u0027classicmodels_star_schema.dim_customers\u0027,\n  \u0027username\u0027 \u003d \u0027postgresuser\u0027,\n  \u0027password\u0027 \u003d \u0027adminpwrd\u0027\n); \n```\n\nExchange the placeholder `\u003cPOSTGRES_ENDPOINT\u003e` (including the brackets `\u003c\u003e`) with the `PostgresEndpoint` value from the Cloudformation Output."
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\nDROP TABLE IF EXISTS dest_customers;\n\nCREATE TABLE dest_customers (\n    `customer_key` INT,\n    `customer_name` STRING,\n    `customer_last_name` STRING,\n    `customer_first_name` STRING,\n    `phone` STRING,\n    `address_line_1` STRING,\n    `address_line_2` STRING,\n    `postal_code` STRING,\n    `city` STRING,\n    `state` STRING,\n    `country` STRING,\n    `creditlimit` FLOAT,\n    `updated_at` TIMESTAMP(3),\n     PRIMARY KEY (`customer_key`)  NOT ENFORCED\n) WITH (\n  \u0027connector\u0027 \u003d \u0027jdbc\u0027,\n  \u0027url\u0027 \u003d \u0027jdbc:postgresql://de-c4w3lab1-rds.cvew4oo6wtxg.us-east-1.rds.amazonaws.com:5432/postgres\u0027,\n  \u0027table-name\u0027 \u003d \u0027classicmodels_star_schema.dim_customers\u0027,\n  \u0027username\u0027 \u003d \u0027postgresuser\u0027,\n  \u0027password\u0027 \u003d \u0027adminpwrd\u0027\n);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.2 - Moving Data\n\nYou can create an INSERT statement to move data from the source table `source_customers` into the output table `dest_customers`. This process will run indefinitely as a Flink Job, constantly reading from the `source_customers` table, and when new data comes, it is inserted in the `dest_customers`.\n\n```sql\nINSERT INTO dest_customers \nSELECT\n    ...\nFROM source_customers;\n```"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2.2.1. Add the table names into the query below to insert into the `dest_customers` the required data from the `source_customers`.\n\nThis will run indefinitely as a Flink Job, so you will keep it running while testing the CDC pipeline. Afterwards, cancel the job using the pause icon in the top right section of the following cell. Wait a few minutes until you see the message: \"**Duration:** ... minutes ... seconds\". After that you can continue."
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\nINSERT INTO dest_customers (customer_key, customer_name, customer_last_name, customer_first_name, phone, address_line_1, address_line_2, postal_code, city, state, country, creditlimit ,updated_at)\nSELECT customerNumber, customerName, contactLastName, contactFirstName, phone, addressLine1, addressLine2, postalCode, city, state, country, creditLimit, event_time\nFROM source_customers;\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2.2.2. To check that the `INSERT` statement is working, specify the table `classicmodels_star_schema.dim_customers` in the queries below. Use the `%psql` Interpreter to run them:"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nselect * from classicmodels_star_schema.dim_customers limit 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nselect count(*) from classicmodels_star_schema.dim_customers ;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.3 - CDC Process\n\nNow that you can see that data in the destination database has been uploaded, you will see how the CDC process works. "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2.3.1. Insert some new data into the MySQL `customers` table and will see that the data is also inserted automatically in the destination database in PostgreSQL at the `dim_customer` table. Make sure that the insertion cell `INSERT INTO dest_customers` (step 2.2.1.) is also running."
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nINSERT INTO classicmodels.customers (customerNumber, customerName, contactLastName, contactFirstName, phone, addressLine1, addressLine2, city, state, postalCode, country, salesRepEmployeeNumber, creditLimit)\nVALUES (498, \u0027Great Toys\u0027, \u0027Smith\u0027, \u0027Jhon\u0027, \u00271234567\u0027, \u0027106 Linden Road Sandown\u0027, \u00274th Floor\u0027, \u0027Singapore\u0027, null, 069045, \u0027Singapore\u0027, 1621, 10000 ), \n(500, \u0027Games \u0026 Toys\u0027, \u0027Murphy\u0027, \u0027 Susan\u0027, \u0027987654321\u0027, \u00275540 North Pendale Street\u0027, \u0027Level 2\u0027, \u0027San Francisco\u0027, \u0027CA\u0027, 94217, \u0027USA\u0027, 1165, 0.0 ),\n(501, \u0027Scaled Toys Distributors, Ltd.\u0027, \u0027Young\u0027, \u0027Mary\u0027, \u0027212557411\u0027, \u00274092 Furth Circle\u0027, \u0027Suite 500\u0027, \u0027NYC\u0027, \u0027NY\u0027, 10022, \u0027USA\u0027, 1504, 200000 );"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nSELECT count(*) FROM classicmodels.customers;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2.3.2. Remember to make sure that the insertion cell `INSERT INTO dest_customers` is currently running, otherwise, run it again and then run the following cell to count the number of rows in the `dim_customers` table at the PostgreSQL database.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nselect count(*) from classicmodels_star_schema.dim_customers;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You should have seen that the new 3 rows have been inserted. Even if you try to delete or update any of those rows in the MySQL source database, those changes will also be propagated to the destination PostgreSQL table."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2.3.3. Let\u0027s delete the customer with customer key `501` as you were told that this company is no longer one of your customers."
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nDELETE FROM classicmodels.customers WHERE customerNumber \u003d 501;\n\nSELECT COUNT(*) FROM classicmodels.customers;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2.3.4. Inspect the result in the `dim_customers` table in PostgreSQL."
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nSELECT COUNT(*) FROM classicmodels_star_schema.dim_customers;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You have seen how insertion and deletion processes are propagated from the source system into the destination system through the CDC process. Now, you can stop the `INSERT INTO dest_customers` cell (step 2.2.1.) to release the task. If the cell associated with this command: \n\n```SQL\nSELECT *\nFROM source_customers limit 5;\n````\n\nis also running (step 2.1.2.), make sure to stop it before continuing with the following section."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3 - Products Dimension\n\nNow you will define the pipeline for the `dim_products` dimensional table, review the original `products` and `productlines` table from the source system, use the `%mysql` interpreter to run the following cells:"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nSELECT * FROM classicmodels.products limit 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nSELECT * FROM classicmodels.productlines limit 10;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "3.1. Define the input tables `source_products` and `source_productlines` using the `%flink.ssql` Interpreter in the following two cells. As in the previous section, the data will be coming from Kafka topics connected to Debezium and the source system. Use the same configuration as the previous table connection (steps 2.1.1. and 2.1.2.), changing the topics to `dbserver1.classicmodels.products` and `dbserver1.classicmodels.productlines` respectively. Don\u0027t forget to add the metadata column `event_time` to each source table."
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\nDROP TABLE IF EXISTS source_products;\n\nCREATE TABLE source_products (\n    `productCode` STRING,\n    `productName` STRING,\n    `productLine` STRING,\n    `productScale` STRING,\n    `productVendor` STRING,\n    `productDescription` STRING,\n    `quantityInStock` INT,\n    `buyPrice` FLOAT,\n    `MSRP` FLOAT,\n    `event_time` TIMESTAMP(3) METADATA FROM \u0027timestamp\u0027,\n    PRIMARY KEY (`productCode`) NOT ENFORCED\n) WITH (\n  \u0027connector\u0027 \u003d \u0027kafka\u0027,\n  \u0027topic\u0027 \u003d \u0027dbserver1.classicmodels.products\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027kafka:9092\u0027,\n  \u0027properties.group.id\u0027 \u003d \u0027testGroup\u0027,\n  \u0027format\u0027 \u003d \u0027debezium-json\u0027,\n  \u0027scan.startup.mode\u0027 \u003d \u0027earliest-offset\u0027,\n  \u0027debezium-json.schema-include\u0027 \u003d \u0027true\u0027\n);\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\nDROP TABLE IF EXISTS source_productlines;\n\nCREATE TABLE source_productlines (\n    `productLine` STRING,\n    `textDescription` STRING,\n    `event_time` TIMESTAMP(3) METADATA FROM \u0027timestamp\u0027,\n    PRIMARY KEY (`productLine`) NOT ENFORCED\n) WITH (\n  \u0027connector\u0027 \u003d \u0027kafka\u0027,\n  \u0027topic\u0027 \u003d \u0027dbserver1.classicmodels.productlines\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027kafka:9092\u0027,\n  \u0027properties.group.id\u0027 \u003d \u0027testGroup\u0027,\n  \u0027format\u0027 \u003d \u0027debezium-json\u0027,\n  \u0027scan.startup.mode\u0027 \u003d \u0027earliest-offset\u0027,\n  \u0027debezium-json.schema-include\u0027 \u003d \u0027true\u0027\n);\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "3.2. Define the `dest_products` table, this will point to the Postgres RDS target database, specifically the `classicmodels_star_schema.dim_products` table. Exchange the placeholder `\u003cPOSTGRES_ENDPOINT\u003e` (including the brackets `\u003c\u003e`) with the `PostgresEndpoint` value from the Cloudformation Output and all of the `None` with the required values."
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\nDROP TABLE IF EXISTS dest_products;\n\nCREATE TABLE dest_products (\n    `product_key` STRING,\n\t`product_name` STRING,\n\t`product_line` STRING,\n\t`product_scale` STRING,\n\t`product_vendor` STRING,\n\t`product_description` STRING,\n\t`product_line_description` STRING,\n\t`updated_at` TIMESTAMP(3),\n     PRIMARY KEY (`product_key`)  NOT ENFORCED\n) WITH (\n  \u0027connector\u0027 \u003d \u0027jdbc\u0027,\n  \u0027url\u0027 \u003d \u0027jdbc:postgresql://de-c4w3lab1-rds.cvew4oo6wtxg.us-east-1.rds.amazonaws.com:5432/postgres\u0027,\n  \u0027table-name\u0027 \u003d \u0027classicmodels_star_schema.dim_products\u0027,\n  \u0027username\u0027 \u003d \u0027postgresuser\u0027,\n  \u0027password\u0027 \u003d \u0027adminpwrd\u0027\n);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "3.3. Now, you will insert into the `dest_products` the required data from joining the `source_products` and `dest_productlines` tables, this will run indefinitely as a Flink Job, so we will keep it running while testing the CDC pipeline. Afterwards, cancel the job using the pause icon in the top right section of the following cell. Wait a few minutes until you see the message: \"**Duration:** ... minutes ... seconds\". After that you can continue."
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\nINSERT INTO dest_products (product_key, product_name, product_line, product_scale, product_vendor, product_description, product_line_description, updated_at)\nSELECT sp.productCode, sp.productName, sp.productLine, sp.productScale, sp.productVendor, sp.productDescription, spl.textDescription, sp.event_time\nFROM source_products AS sp\nJOIN source_productlines AS spl on sp.productLine \u003d spl.productLine\n;\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "3.4. Let\u0027s check that the `INSERT` statement is working. Use the %psql Interpreter to run the following cells:"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nselect * from classicmodels_star_schema.dim_products limit 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nselect count(*) from classicmodels_star_schema.dim_products;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "3.5. Now, let\u0027s insert and delete some rows into the `products` table in the MySQL source database and see how those changes are propagated into the PostgreSQL database through the CDC process. Currently, there are 110 products in the source database. Let\u0027s insert some new products. Wait a few minutes until you see the message: \"**Duration:** ... minutes ... seconds\". After that you can continue."
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nINSERT INTO classicmodels.products (productCode, productName, productLine, productScale, productVendor, productDescription, quantityInStock, buyPrice, MSRP)\nVALUES  (\u0027S700_5000\u0027, \u0027Santisima Trinidad\u0027, \u0027Ships\u0027, \u00271:700\u0027, \u0027Autoart Studio Design\u0027, \u0027All wood with canvas sails. Measures 31 1/2 inches in Length, 22 inches High and 4 3/4 inches Wide. Many extras. Was a Spanish first-rate ship of the line and was the largest warship in the world when launched on 1769.\u0027, 500, 40.0, 75.0), \n(\u0027S700_5001\u0027, \u0027Couronne\u0027, \u0027Ships\u0027, \u00271:700\u0027, \u0027Unimax Art Galleries\u0027, \u0027Measures 30 inches Long x 27 1/2 inches High x 4 3/4 inches Wide.  Many extras including rigging, long boats, pilot house, anchors, etc. Comes with three masts, all square-rigged.\u0027, 10, 31.2, 50.0);"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nSELECT count(*) FROM classicmodels.products;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "3.6. With the new two rows inserted, let\u0027s check the `dim_products` dimension table at PostgreSQL. Remember that the cell performing the `INSERT INTO dest_products` command (step 3.3.) should be running before executing the following cell:"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nSELECT count(*) FROM classicmodels_star_schema.dim_products;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "3.7. Now, it seems like the information provided for the product with code `\u0027S700_5001\u0027` was incorrect and the actual `productScale` is `\u00271:500\u0027` and the actual `productVendor` is `\u0027Autoart Studio Design\u0027`. Let\u0027s perform an update process in the MySQL table and check the result in PostgreSQL:"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nUPDATE classicmodels.products \nSET productScale \u003d \u00271:500\u0027, productVendor \u003d \u0027Autoart Studio Design\u0027\nWHERE productCode\u003d\u0027S700_5001\u0027;\n\nSELECT * FROM classicmodels.products WHERE productCode\u003d\u0027S700_5001\u0027;"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nSELECT * FROM classicmodels_star_schema.dim_products WHERE product_key\u003d\u0027S700_5001\u0027;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You have seen in this example how the CDC process is able to take into account the updates performed in the sources system and propagate them into the destination system. Remember to stop the `INSERT INTO dest_products` process (step 3.3.) above before continuing with the next section in order to release the Flink\u0027s task slots."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4 - Fact Table\n\nFinally, you will define the pipeline for the `fact_orders` fact table, review the original `orders` and `orderdetails` table from the source system, use the `%mysql` interpreter to run the following cells:"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nSELECT * FROM classicmodels.orders limit 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%mysql\nSELECT * FROM classicmodels.orderdetails limit 10;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "4.1. Define the input tables `source_orders` and `source_orderdetails` using the `%flink.ssql` Interpreter. As in the previous sections, use a similar configuration as the previous table connections, just change the topics to `dbserver1.classicmodels.orders` and `dbserver1.classicmodels.orderdetails` respectively. There will be the same metadata column `event_time` added to each source table."
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\nDROP TABLE IF EXISTS source_orders;\n\nCREATE TABLE source_orders (\n    orderNumber INT, \n    orderDate BIGINT,\n    requiredDate BIGINT,\n    shippedDate BIGINT,\n    status STRING,\n    comments STRING,\n    customerNumber INT,\n    event_time TIMESTAMP(3) METADATA FROM \u0027timestamp\u0027,\n    PRIMARY KEY ( orderNumber) NOT ENFORCED\n) WITH (\n  \u0027connector\u0027 \u003d \u0027kafka\u0027,\n  \u0027topic\u0027 \u003d \u0027dbserver1.classicmodels.orders\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027kafka:9092\u0027,\n  \u0027properties.group.id\u0027 \u003d \u0027testGroup\u0027,\n  \u0027format\u0027 \u003d \u0027debezium-json\u0027,\n  \u0027scan.startup.mode\u0027 \u003d \u0027earliest-offset\u0027,\n  \u0027debezium-json.schema-include\u0027 \u003d \u0027true\u0027\n);\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\nDROP TABLE IF EXISTS source_orderdetails;\n\nCREATE TABLE source_orderdetails (\n`orderNumber` INT, \n`productCode` STRING,\n`quantityOrdered` INT,\n`priceEach` FLOAT,\n`orderLineNumber` INT,\n`event_time` TIMESTAMP(3) METADATA FROM \u0027timestamp\u0027,\n  PRIMARY KEY ( orderNumber, productCode) NOT ENFORCED\n) WITH (\n  \u0027connector\u0027 \u003d \u0027kafka\u0027,\n  \u0027topic\u0027 \u003d \u0027dbserver1.classicmodels.orderdetails\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027kafka:9092\u0027,\n  \u0027properties.group.id\u0027 \u003d \u0027testGroup\u0027,\n  \u0027format\u0027 \u003d \u0027debezium-json\u0027,\n  \u0027scan.startup.mode\u0027 \u003d \u0027earliest-offset\u0027,\n  \u0027debezium-json.schema-include\u0027 \u003d \u0027true\u0027\n);\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "4.2. Define the `dest_factorders` table, this will point to the Postgres RDS target database, specifically the `classicmodels_star_schema.fact_orders` table. Change the placeholder `\u003cPOSTGRES_ENDPOINT\u003e` (including the brackets `\u003c\u003e`) with the `PostgresEndpoint` value from the Cloudformation Output and all of the `None`s with the required values."
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\nDROP TABLE IF EXISTS dest_factorders;\n\nCREATE TABLE dest_factorders (\n    order_key INT, \n    order_line_key INT,\n    customer_key INT,\n    product_key STRING,\n    order_date BIGINT,\n    order_required_date BIGINT,\n    order_shipped_date BIGINT,\n    quantity_ordered INT,\n    product_price FLOAT,\n    profit FLOAT,\n    updated_at TIMESTAMP(3),\n    PRIMARY KEY ( order_key, order_line_key, customer_key, product_key) NOT ENFORCED\n) WITH (\n  \u0027connector\u0027 \u003d \u0027jdbc\u0027,\n  \u0027url\u0027 \u003d \u0027jdbc:postgresql://de-c4w3lab1-rds.cvew4oo6wtxg.us-east-1.rds.amazonaws.com:5432/postgres\u0027,\n  \u0027table-name\u0027 \u003d \u0027classicmodels_star_schema.fact_orders\u0027,\n  \u0027username\u0027 \u003d \u0027postgresuser\u0027,\n  \u0027password\u0027 \u003d \u0027adminpwrd\u0027\n);\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "4.3. Run the `INSERT` query that creates the fact table, there is a field called `profit` that is calculated by subtracting from the price of each product the price for what the product was bought. Wait a moment until you see the message:\n**Duration:** and the counter has measured a few seconds before you can continue."
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\nINSERT INTO dest_factorders (order_key, order_line_key, customer_key, product_key, order_date, order_required_date, order_shipped_date, quantity_ordered, product_price, profit, updated_at)\nSELECT so.orderNumber, sod.orderLineNumber, so.customerNumber, sod.productCode, so.orderDate, so.requiredDate, so.shippedDate, sod.quantityOrdered, sod.priceEach, (sod.priceEach - sp.buyPrice) * sod.quantityOrdered, so.event_time\nFROM source_orders AS so\nJOIN source_orderdetails AS sod ON so.orderNumber \u003d sod.orderNumber\nJOIN source_products AS sp ON sod.productCode \u003d sp.productCode\n;\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "4.4. Let\u0027s check that the `INSERT` statement is working. Use the `%psql` Interpreter to run the following cells:"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nselect * from classicmodels_star_schema.fact_orders limit 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%psql\nselect count(*) from classicmodels_star_schema.fact_orders;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "4.5. Before finishing the lab, remember to stop the cell associated with the process: `INSERT INTO dest_factorders` (step 4.3.)."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In this lab, you explored the implementation of Change Data Capture (CDC) using Apache Flink and Debezium. The combination of Debezium and Flink offers a powerful solution for capturing and processing data changes, making it easier to maintain consistency and derive insights from continuously evolving datasets."
    }
  ]
}